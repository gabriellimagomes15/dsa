{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.5.0-cp36-cp36m-win_amd64.whl (31.1MB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tensorflow)\n",
      "Collecting protobuf>=3.4.0 (from tensorflow)\n",
      "  Downloading protobuf-3.5.1-py2.py3-none-any.whl (388kB)\n",
      "Collecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)\n",
      "  Downloading tensorflow_tensorboard-1.5.1-py3-none-any.whl (3.0MB)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading absl-py-0.1.10.tar.gz (79kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Collecting html5lib==0.9999999 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\n",
      "Collecting bleach==1.5.0 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)\n",
      "  Downloading Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "Building wheels for collected packages: absl-py, html5lib\n",
      "  Running setup.py bdist_wheel for absl-py: started\n",
      "  Running setup.py bdist_wheel for absl-py: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gabri\\AppData\\Local\\pip\\Cache\\wheels\\45\\07\\0e\\6880381ca521796cf6cc18ba4ab502c2232e5777099b4df4ae\n",
      "  Running setup.py bdist_wheel for html5lib: started\n",
      "  Running setup.py bdist_wheel for html5lib: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gabri\\AppData\\Local\\pip\\Cache\\wheels\\6f\\85\\6c\\56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built absl-py html5lib\n",
      "Installing collected packages: protobuf, html5lib, bleach, markdown, tensorflow-tensorboard, absl-py, tensorflow\n",
      "  Found existing installation: html5lib 0.999999999\n",
      "    Uninstalling html5lib-0.999999999:\n",
      "      Successfully uninstalled html5lib-0.999999999\n",
      "  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "Successfully installed absl-py-0.1.10 bleach-1.5.0 html5lib-0.9999999 markdown-2.6.11 protobuf-3.5.1 tensorflow-1.5.0 tensorflow-tensorboard-1.5.1\n",
      "Requirement already satisfied: tflearn in c:\\users\\gabri\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tflearn)\n",
      "Requirement already satisfied: numpy in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tflearn)\n",
      "Requirement already satisfied: Pillow in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tflearn)\n",
      "Requirement already satisfied: olefile in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from Pillow->tflearn)\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "!pip install tensorflow\n",
    "!pip install tflearn\n",
    "import tflearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install tensorflow==1.2.0 --ignore-installed\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a table structure to hold the different punctuation used\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                    if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "# initialize the stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# variable to hold the Json data read from the file\n",
    "data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': ['what time is it?', 'how long has it been since we started?', \"that's a long time ago\", ' I spoke to you last week', ' I saw you yesterday'], 'sorry': [\"I'm extremely sorry\", 'did he apologize to you?', \"I shouldn't have been rude\"], 'greeting': ['Hello there!', 'Hey man! How are you?', 'hi'], 'farewell': ['It was a pleasure meeting you', 'Good Bye.', 'see you soon', 'I gotta go now.'], 'age': [\"what's your age?\", 'How old are you?', \"I'm a couple of years older than her\", 'You look aged!']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the json file and load the training data\n",
    "with open('data.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what time is it\n",
      "tokenized words:  ['what', 'time', 'is', 'it']\n",
      "how long has it been since we started\n",
      "tokenized words:  ['how', 'long', 'has', 'it', 'been', 'since', 'we', 'started']\n",
      "thats a long time ago\n",
      "tokenized words:  ['thats', 'a', 'long', 'time', 'ago']\n",
      " I spoke to you last week\n",
      "tokenized words:  ['I', 'spoke', 'to', 'you', 'last', 'week']\n",
      " I saw you yesterday\n",
      "tokenized words:  ['I', 'saw', 'you', 'yesterday']\n",
      "Im extremely sorry\n",
      "tokenized words:  ['Im', 'extremely', 'sorry']\n",
      "did he apologize to you\n",
      "tokenized words:  ['did', 'he', 'apologize', 'to', 'you']\n",
      "I shouldnt have been rude\n",
      "tokenized words:  ['I', 'shouldnt', 'have', 'been', 'rude']\n",
      "Hello there\n",
      "tokenized words:  ['Hello', 'there']\n",
      "Hey man How are you\n",
      "tokenized words:  ['Hey', 'man', 'How', 'are', 'you']\n",
      "hi\n",
      "tokenized words:  ['hi']\n",
      "It was a pleasure meeting you\n",
      "tokenized words:  ['It', 'was', 'a', 'pleasure', 'meeting', 'you']\n",
      "Good Bye\n",
      "tokenized words:  ['Good', 'Bye']\n",
      "see you soon\n",
      "tokenized words:  ['see', 'you', 'soon']\n",
      "I gotta go now\n",
      "tokenized words:  ['I', 'got', 'ta', 'go', 'now']\n",
      "whats your age\n",
      "tokenized words:  ['whats', 'your', 'age']\n",
      "How old are you\n",
      "tokenized words:  ['How', 'old', 'are', 'you']\n",
      "Im a couple of years older than her\n",
      "tokenized words:  ['Im', 'a', 'couple', 'of', 'years', 'older', 'than', 'her']\n",
      "You look aged\n",
      "tokenized words:  ['You', 'look', 'aged']\n"
     ]
    }
   ],
   "source": [
    "# get a list of all categories to train for\n",
    "categories = list(data.keys())\n",
    "words = []\n",
    "# a list of tuples with words in the sentence and category name\n",
    "docs = []\n",
    "\n",
    "for each_category in data.keys():\n",
    "    for each_sentence in data[each_category]:\n",
    "        # remove any punctuation from the sentence\n",
    "        each_sentence = remove_punctuation(each_sentence)\n",
    "        print(each_sentence)\n",
    "        # extract words from each sentence and append to the word list\n",
    "        w = nltk.word_tokenize(each_sentence)\n",
    "        print(\"tokenized words: \", w)\n",
    "        words.extend(w)\n",
    "        docs.append((w, each_category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['what', 'time', 'is', 'it'], 'time'), (['how', 'long', 'has', 'it', 'been', 'since', 'we', 'started'], 'time'), (['thats', 'a', 'long', 'time', 'ago'], 'time'), (['I', 'spoke', 'to', 'you', 'last', 'week'], 'time'), (['I', 'saw', 'you', 'yesterday'], 'time'), (['Im', 'extremely', 'sorry'], 'sorry'), (['did', 'he', 'apologize', 'to', 'you'], 'sorry'), (['I', 'shouldnt', 'have', 'been', 'rude'], 'sorry'), (['Hello', 'there'], 'greeting'), (['Hey', 'man', 'How', 'are', 'you'], 'greeting'), (['hi'], 'greeting'), (['It', 'was', 'a', 'pleasure', 'meeting', 'you'], 'farewell'), (['Good', 'Bye'], 'farewell'), (['see', 'you', 'soon'], 'farewell'), (['I', 'got', 'ta', 'go', 'now'], 'farewell'), (['whats', 'your', 'age'], 'age'), (['How', 'old', 'are', 'you'], 'age'), (['Im', 'a', 'couple', 'of', 'years', 'older', 'than', 'her'], 'age'), (['You', 'look', 'aged'], 'age')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "#print(words)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    # initialize our bag of words(bow) for each document in the list\n",
    "    bow = []\n",
    "    # list of tokenized words for the pattern\n",
    "    token_words = doc[0]\n",
    "    # stem each word\n",
    "    token_words = [stemmer.stem(word.lower()) for word in token_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bow.append(1) if w in token_words else bow.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(doc[1])] = 1\n",
    "\n",
    "    # our training set will contain a the bag of words model and the output row that tells\n",
    "    # which catefory that bow belongs to.\n",
    "    training.append([bow, output_row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle our features and turn into np.array as tensorflow  takes in numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# trainX contains the Bag of words and train_y contains the label/ category\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4499  | total loss: \u001b[1m\u001b[32m0.00402\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 1500 | loss: 0.00402 - acc: 1.0000 -- iter: 16/19\n",
      "Training Step: 4500  | total loss: \u001b[1m\u001b[32m0.00404\u001b[0m\u001b[0m | time: 0.009s\n",
      "| Adam | epoch: 1500 | loss: 0.00404 - acc: 1.0000 -- iter: 19/19\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1500, batch_size=8, show_metric=True)\n",
    "#model.save('model.tflearn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tf_record(sentenceL):\n",
    "    #print(sentenceL)\n",
    "    global words\n",
    "    retorno = []\n",
    "    if type(sentenceL) == list :\n",
    "        for sentence in sentenceL:\n",
    "            #print(sentence)\n",
    "            # tokenize the pattern\n",
    "            sentence_words = nltk.word_tokenize(sentence)\n",
    "            # stem each word\n",
    "            sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "            # bag of words\n",
    "            bow = [0]*len(words)\n",
    "            for s in sentence_words:\n",
    "                for i, w in enumerate(words):\n",
    "                    if w == s:\n",
    "                        bow[i] = 1\n",
    "            retorno.append(bow)\n",
    "\n",
    "        return(np.array(retorno))\n",
    "    else:\n",
    "        sentence = sentenceL\n",
    "        # tokenize the pattern\n",
    "        sentence_words = nltk.word_tokenize(sentence)\n",
    "        # stem each word\n",
    "        sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "        # bag of words\n",
    "        bow = [0]*len(words)\n",
    "        for s in sentence_words:\n",
    "            for i, w in enumerate(words):\n",
    "                if w == s:\n",
    "                    bow[i] = 1\n",
    "        retorno.append(bow)\n",
    "\n",
    "        return(np.array(bow))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test the mdodel for a few sentences:\n",
    "# the first two sentences are used for training, and the last two sentences are not present in the training data.\n",
    "sent_1 = \"what time is it?\"\n",
    "sent_2 = \"I gotta go now\"\n",
    "sent_3 = \"do you know the time now?\"\n",
    "sent_4 = \"you must be a couple of years older then her!\"\n",
    "\n",
    "sents = [\"what time is it?\",\"I gotta go now\",\"do you know the time now?\",\"you must be a couple of years older then her!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['time', 'sorry', 'greeting', 'farewell', 'age']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.argmax(model.predict([get_tf_record(sent_1)])))\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "farewell\n",
      "farewell\n",
      "age\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we can start to predict the results for each of the 4 sentences\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_1)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_2)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_3)]))])\n",
    "print(categories[np.argmax(model.predict([get_tf_record(sent_4)]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_tf_record(sents)\n",
    "labels = [0,3,3,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.95620549e-01   2.26564473e-03   9.98753364e-08   2.11372017e-03\n",
      "    4.43430892e-08]\n",
      " [  4.29353713e-05   3.22086047e-09   1.45874766e-03   9.98498321e-01\n",
      "    8.65566552e-09]\n",
      " [  2.90638983e-01   1.47789922e-02   1.36470152e-02   6.79553807e-01\n",
      "    1.38119771e-03]\n",
      " [  9.49604284e-11   1.21153098e-05   9.94933071e-04   8.00174149e-09\n",
      "    9.98992980e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "#print(categories[np.argmax(model.predict([get_tf_record(sent_1)]))])\n",
    "#print(model.predict([get_tf_record(sent_1)]))\n",
    "#print(np.argmax(get_tf_record([sent_1])))\n",
    "#print(get_tf_record(sent_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 3, 4]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = list(np.argmax(pred,axis=1))\n",
    "#pred.max(axis=0)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels,pred_y)\n",
    "#print(labels, pred_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
