{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 10 - Processamento de Linguagem Natural</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** Este Jupyter Notebook foi atualizado para a versão 3.6.1. da Linguagem Python em 05/07/2017 ******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "\n",
    "O modelo de \"saco de palavras\" é uma representação simplificada usada no processamento de linguagem natural e recuperação de informação. Neste modelo, um texto (como uma sentença ou um documento) é representado como o saco (multiset) de suas palavras, desconsiderando a gramática e até a ordem das palavras, mas mantendo a multiplicidade.\n",
    "\n",
    "Na classificação de documentos, um saco de palavras é um vetor esparso de ocorrência de contagens de palavras; Ou seja, um histograma esparso sobre o vocabulário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando um Dataset de um Site de E-commerce (em português)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset\n",
    "corpus = list()\n",
    "with gzip.open('ecommerce.json.gz') as fp:\n",
    "    for line in fp:\n",
    "        entry = line.decode('utf8')\n",
    "        corpus.append(json.loads(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (corpus[0]['descr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim - Modelagem de Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por enquanto disponíve, apenas para Python 2.7\n",
    "#!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "print (gensim.summarization.summarize(corpus[0]['descr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo um classificador para produtos e categorias (considerando apenas os 10 mil primeiros produtos)\n",
    "dataset = list()\n",
    "for entry in corpus[:50000]:\n",
    "    if 'cat' in entry:\n",
    "        dataset.append( (entry['name'], entry['cat'].lower().strip()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantas categorias distintas nós temos e quantos itens por categoria?\n",
    "from collections import Counter\n",
    "counter = Counter([cat for prod, cat in dataset])\n",
    "pprint(counter.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo um Classificador SVM com Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in corpus[:2]:\n",
    "    print(i['title'].lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = [ 'r' + \"'\\b\" + s + \"\\b'\" + '|' for s in stopwords]\n",
    "stopwords = ''.join(stopwords)\n",
    "#stopwords = '['+ stopwords + ']'\n",
    "\n",
    "for t, doc in dataset[:15]:\n",
    "    for w in t.split(' '):\n",
    "        if w not in stopwords:\n",
    "            t = t + w\n",
    "            \n",
    "    # stem each word\n",
    "    #sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo o modelo SVM com Pipeline\n",
    "modelo = Pipeline([('vect', TfidfVectorizer()), ('clf', SVC(kernel = 'linear', probability = True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto para Normalização dos labels\n",
    "\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo dados e labels\n",
    "data = [prod for prod, cat in dataset]\n",
    "labels = [cat for prod, cat in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização dos labels\n",
    "target = encoder.fit_transform(labels)\n",
    "set(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Items\n",
    "encoder.classes_.item(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit do modelo\n",
    "modelo.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelo.predict(test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print( confusion_matrix(target, pred) ) \n",
    "print( '\\n acuracia : ', accuracy_score(test_y, pred) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevendo a categoria a partir da descrição\n",
    "print (encoder.classes_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidades de um produto\n",
    "probs = modelo.predict_proba([\"not recommend\",\"good peoples\",\"wors job\"])\n",
    "print(probs)\n",
    "for i in probs:\n",
    "    print(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabidades de categorias para o objeto Ventilador\n",
    "guess = [(class_, probs.item(n)) for n, class_ in enumerate(encoder.classes_)]\n",
    "pprint(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabidade ajustada de categorias para o objeto Ventilador\n",
    "from operator import itemgetter\n",
    "for cat, proba in sorted(guess, key = itemgetter(1), reverse = True):\n",
    "    print ('{}: {:.4f}'.format(cat, proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strings = [\"Important text not, me i      !Comment that could be removed\", \"not Other String\"]\n",
    "[re.sub('i, \"\", x) for x in strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=http://facebook.com/dsacademy>facebook.com/dsacademybr</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names #prints all the categories\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\"\"\"#print(count_vect)\n",
    "x = []\n",
    "y = []\n",
    "for d in dataset:\n",
    "    x.append(d[0])\n",
    "    y.append(d[1])\n",
    "    \n",
    "test = [ i['title'] for i in corpus[20000:30000]]\n",
    "test_y = [ i['recommend'] for i in corpus[20000:30000]]\n",
    "\n",
    "\"\"\"\n",
    "X_train_counts = count_vect.fit_transform(data)\n",
    "print(X_train_counts.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, twenty_train.target)\n",
    "#print(X, '\\n\\n', X[2:3])\n",
    "\"\"\"\n",
    "print(test[1],test_y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(test[1],test_y[1] )  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type( [1,2,.6]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "#print(twenty_test.data[0])\n",
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "l = pd.DataFrame(data=d)\n",
    "print(l)\n",
    "\n",
    "#predicted = clf.predict(l) \n",
    "clf.predict(twenty_test.data)\n",
    "#print(predicted)\n",
    "#np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, n_iter=5, random_state=42)),\n",
    " ])\n",
    "text_clf_svm\n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DADOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('~/Documents/dsa/data/reviewFinal.csv',encoding='latin-1')\n",
    "corpus = pd.DataFrame()\n",
    "listTitle = []\n",
    "listRecom = []\n",
    "for i, r in dados.iterrows():\n",
    "    #print(i)\n",
    "    if r['recommend'] != 'NI' and r['title'] != \"\" and r['title'] is not np.nan and r['site'] == \"glassdoor\":\n",
    "        #corpus.append(r)\n",
    "        #corpus['title'] = r['title']\n",
    "        #corpus['recommend'] = r['recommend']\n",
    "        listTitle.append(r['title'])\n",
    "        listRecom.append(r['recommend'])\n",
    "\n",
    "\"\"\"\n",
    "for entry in corpus[:rows_train]:\n",
    "    #print(entry)\n",
    "    dataset.append( (entry['title'].lower().strip(), entry['recommend'].lower().strip()) )\n",
    "\"\"\"\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['title'] = listTitle\n",
    "corpus['recommend'] = listRecom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GERANDO LINHAS PARA DADOS DE TREINO E TESTE\n",
    "\n",
    "x = [ i for i in range(len(corpus))]\n",
    "shuffle(x)\n",
    "\n",
    "rows_train =  x[0:round( len(corpus) * .75)] #25000\n",
    "rows_test  = x[round( len(corpus) * .75):(len(corpus))]\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a =\n",
    "print(len(rows_train)+len(rows_test), len(corpus) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIMPANDO TEXTO\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "sClear = []\n",
    "tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "i = 0\n",
    "for i,text in corpus.iterrows():    \n",
    "    #print(text['title'].split())\n",
    "    clear = \"\"\n",
    "    for word in text['title'].split():        \n",
    "        word = word.lower() ## CONVERTENDO PARA MINUSCULO\n",
    "        word = word.translate(tr) ##REMOVENDO PONTUAÇÕES        \n",
    "        word = re.sub(\"\\d\",\" \",word) ## REMOVENDO NUMEROS\n",
    "        word = stemmer.stem(word) ## STEMM \n",
    "        \n",
    "        clear = clear + ' ' + word\n",
    "        clear = re.sub(\"\\s+\",\" \",clear) ## REMOVENDO ESPAÇOS DUPLICADOS\n",
    "        \n",
    "        \n",
    "    sClear.append(clear)\n",
    "    text['title'] = clear\n",
    "    i = i + 1    \n",
    "\n",
    "#print(sClear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1} {0, 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "train   = corpus.iloc[rows_train, [0]]\n",
    "train   = train['title'].tolist()\n",
    "train_y = corpus.iloc[rows_train, [1] ] \n",
    "train_y = encoder.fit_transform(train_y)\n",
    "\n",
    "test   = corpus.iloc[rows_test, [0]]\n",
    "test   = test['title'].tolist()\n",
    "test_y = corpus.iloc[rows_test, [1]]\n",
    "test_y = encoder.fit_transform(test_y)\n",
    "\n",
    "print(set(train_y), set(test_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELOS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES MULTINOMIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64460, 8943)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing\n",
    "count_vect     = CountVectorizer(stop_words= stopwords.words('english'))\n",
    "X_train_counts = count_vect.fit_transform(train)\n",
    "count_vect.vocabulary_.get(u'algorithm')\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64460, 8943)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mesmo resultado da célula anterior, mas combinando as funções\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf     = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões\n",
    "X_new_counts = count_vect.transform(test)\n",
    "X_new_tfidf  = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "#print(predicted)\n",
    "#for doc, category in zip(docs_new, predicted):    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2906  4252]\n",
      " [  923 13405]]\n",
      "\n",
      " acuracia :  0.759145490086568\n"
     ]
    }
   ],
   "source": [
    "print( confusion_matrix(test_y, predicted) ) \n",
    "print( '\\n acuracia : ', accuracy_score(test_y, predicted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINOMIAL NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(X_train_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictBIN = nb.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2906  4252]\n",
      " [  923 13405]]\n",
      "\n",
      " acuracia :  0.757144186912408\n"
     ]
    }
   ],
   "source": [
    "print( confusion_matrix(test_y, predicted) ) \n",
    "print( '\\n acuracia : ', accuracy_score(test_y, predictBIN) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo  = Pipeline([('vect', TfidfVectorizer(stop_words=stopwords.words('english'))), \n",
    "                    ('clf', SVC(kernel = 'linear', probability = True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSVC = modelo.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3176  3982]\n",
      " [ 1205 13123]]\n",
      "\n",
      " acuracia :  0.7585869868751746\n"
     ]
    }
   ],
   "source": [
    "print( confusion_matrix(test_y, predictSVC) ) \n",
    "print( '\\n acuracia : ', accuracy_score(test_y, predictSVC) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  915  6243]\n",
      " [  276 14052]]\n",
      "\n",
      " acuracia :  0.6965931304104999\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer(stopwords.words('english'))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=500, tol=None)),\n",
    "])\n",
    "text_clf.fit(train, train_y)  \n",
    "\n",
    "predicted = text_clf.predict(test)\n",
    "print( confusion_matrix(test_y, predicted) ) \n",
    "print( '\\n acuracia : ', accuracy_score(test_y, predicted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "  Downloading tflearn-0.3.2.tar.gz (98kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 850kB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from tflearn)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from tflearn)\n",
      "Requirement already satisfied: Pillow in /anaconda3/lib/python3.6/site-packages (from tflearn)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Running setup.py bdist_wheel for tflearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/gabriel.gomes/Library/Caches/pip/wheels/fb/06/72/0478c938ca315c6fddcce8233b80ec91a115ce4496a27e8c90\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/akshaypai/tfClassifier/blob/master/text_classification/classify_text.py\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "!pip install tflearn \n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a table structure to hold the different punctuation used\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                    if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "\n",
    "# method to remove punctuations from sentences.\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "# initialize the stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# variable to hold the Json data read from the file\n",
    "data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#for each_category in data.keys():\\n    for each_sentence in data[each_category]:\\n        # remove any punctuation from the sentence\\n        each_sentence = remove_punctuation(each_sentence)\\n        print(each_sentence)\\n        # extract words from each sentence and append to the word list\\n        w = nltk.word_tokenize(each_sentence)\\n        print(\"tokenized words: \", w)\\n        words.extend(w)\\n        docs.append((w, each_category))\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of all categories to train for\n",
    "categories = train_y\n",
    "words = []\n",
    "# a list of tuples with words in the sentence and category name\n",
    "docs = []\n",
    "\n",
    "c = corpus[0:10]\n",
    "for i,text in c.iterrows():\n",
    "    #print(text['title'])\n",
    "    #print(remove_punctuation(text['title']))\n",
    "    w = nltk.word_tokenize(text['title'])\n",
    "    #print(words)\n",
    "    words.extend(w)\n",
    "    docs.append((w, text['recommend']))\n",
    "\"\"\"\n",
    "#for each_category in data.keys():\n",
    "    for each_sentence in data[each_category]:\n",
    "        # remove any punctuation from the sentence\n",
    "        each_sentence = remove_punctuation(each_sentence)\n",
    "        print(each_sentence)\n",
    "        # extract words from each sentence and append to the word list\n",
    "        w = nltk.word_tokenize(each_sentence)\n",
    "        print(\"tokenized words: \", w)\n",
    "        words.extend(w)\n",
    "        docs.append((w, each_category))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['al', 'awesom', 'but', 'cli', 'commod', 'compan', 'cult', 'del', 'develop', 'emc', 'employ', 'endless', 'fantast', 'for', 'glob', 'good', 'gre', 'man', 'marry', 'not', 'plac', 'poss', 'review', 'salar', 'seny', 'softw', 'technolog', 'their', 'to', 'work']\n",
      "[(['great', 'technolog', 'compani', 'great', 'cultur'], 'Yes'), (['dell', 'emc', 'marriag', 'awesom', 'for', 'their', 'client', 'but', 'not', 'all', 'employe'], 'Yes'), (['review'], 'Yes'), (['great', 'place', 'to', 'work'], 'Yes'), (['salari'], 'Yes'), (['senior', 'softwar', 'develop'], 'Yes'), (['fantast', 'employ'], 'Yes'), (['global', 'commod', 'manag'], 'No'), (['good', 'place', 'to', 'work'], 'Yes'), (['endless', 'possibl'], 'Yes')]\n"
     ]
    }
   ],
   "source": [
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print(words)\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict_keys' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-597804bd6c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"gabriel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"lima\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict_keys' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "d = {'d':\"gabriel\", 'r':\"lima\"}\n",
    "print(type(d.keys().index(d[1]) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-a819da69e86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moutput_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moutput_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# our training set will contain a the bag of words model and the output row that tells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "\n",
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(set(train_y))\n",
    "\n",
    "\n",
    "for doc in docs:\n",
    "    # initialize our bag of words(bow) for each document in the list\n",
    "    bow = []\n",
    "    # list of tokenized words for the pattern\n",
    "    token_words = doc[0]\n",
    "    # stem each word\n",
    "    token_words = [stemmer.stem(word.lower()) for word in token_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bow.append(1) if w in token_words else bow.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(doc[1])] = 1\n",
    "\n",
    "    # our training set will contain a the bag of words model and the output row that tells\n",
    "    # which catefory that bow belongs to.\n",
    "    training.append([bow, output_row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
